{
  "hash": "afecd9edd62d2722d16c27e480d19a13",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mining Text From Course Descriptions\"\nsubtitle: Using R to examine words in my 70+ college course history.\nabstract: \"In this post, I describe a recent workflow that I ran on my college course descriptions to explore the most common words and word associations from my archive of college course.\"\ndate: 2024-05-21\ndate-modified: today\ncategories:\n  - R\n  - Text Mining\n  - Word Cloud\nkeywords:\n  - \"Graduate School\"\nimage: img-word-cloud.png\nimage-alt: \"An image of a word cloud.\"\ndraft: false\n---\n\n\n## Introduction\n\n![](img-word-cloud.png){.quarto-cover-image}\n\nThis month (May), I wrapped up my first year as a graduate student which also brought my list of college courses up past 70! As the bags under my eyes can tell you, it has been a long road to this point, with much still left to go. I wanted to take some time to look back and reflect on the courses I have experienced in my higher education journey thus far by visualizing some course description data.\n\n### Why course descriptions?\n\nCollege courses all come with a syllabus that lays out the course structure, objectives, and usually a brief official description. These descriptions detail the theory, methods, or general topics of each course and are officially registered with the Registrar. The main reason behind my decision to delve into these descriptions was finally completing my spreadsheet of courses over my academic career. Why did I bother with this? Well, I'm in the process of applying for various professional certifications, many of which require a record of all relevant courses taken. For example, I've been eyeing [The Wildlife Society Certification Program](https://wildlife.org/certification-programs/) for some time now.\n\n## Workflow - Word Cloud\n\nThe steps I took for this process came largely from [this blog post](http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know), which was extremely helpful in understanding these text mining packages in R.\n\n### 1. Data Processing\n\nThe first step we need to do is install and/or load all the required packages. This will include several new-to-me packages like `tm` and `wordcloud` along with some familiar ones like `RColorBrewer`.\n\n\n\n\n::: {.cell filename=''}\n\n```{.r .cell-code}\n# Optional install packages, if needed ----\n# install.packages(\"tm\")  # for text mining\n# install.packages(\"SnowballC\") # for text stemming\n# install.packages(\"wordcloud\") # word-cloud generator \n# install.packages(\"RColorBrewer\") # color palettes\n\n# Load required packages, once installed ----\nlibrary(\"tm\")\nlibrary(\"SnowballC\")\nlibrary(\"wordcloud\")\nlibrary(\"RColorBrewer\")\n```\n:::\n\n\n\n### 2. Data Import\n\nNext, we need to get our data into R and format everything as a `tm::Corpus` dataset that can be transformed in later steps.\n\n```r\n# Load the text\ntext <- base::readLines(file.choose())\n\n# Load the data as a corpus\ndocs <- tm::Corpus(VectorSource(text))\n```\n### 3. Data Wrangling\n\n```r\n# Text transformation ----\ntoSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\ndocs <- tm_map(docs, toSpace, \"/\")\ndocs <- tm_map(docs, toSpace, \"@\")\ndocs <- tm_map(docs, toSpace, \"\\\\|\")\n\n# Text cleaning ----\n\n# Convert the text to lower case\ndocs <- tm_map(docs, content_transformer(tolower))\n# Remove numbers\ndocs <- tm_map(docs, removeNumbers)\n# Remove english common stopwords\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\n# Remove your own stop word\n# specify your stopwords as a character vector\ndocs <- tm_map(docs, removeWords, c(\"course\", \"will\")) \n# Remove punctuations\ndocs <- tm_map(docs, removePunctuation)\n# Eliminate extra white spaces\ndocs <- tm_map(docs, stripWhitespace)\n```\n\n### 4. Creating Matrix\n\n```r\n# Build a Term-Document Matrix ----\ndtm <- TermDocumentMatrix(docs)\nm <- as.matrix(dtm)\nv <- sort(rowSums(m),decreasing=TRUE)\nd <- data.frame(word = names(v),freq=v)\nhead(d, 10)\n```\n\n### 5. Visualization\n\n```r\n# Create and save word cloud\nset.seed(1234)\ngrDevices::png(filename = \"./word_plot.png\",\n               width = 1000,\n               height = 1000,\n               units = \"px\",\n               res = 150)\nwordcloud::wordcloud(words = d$word, freq = d$freq, min.freq = 1,\n          max.words=200, random.order=FALSE, rot.per=0.35, \n          colors=brewer.pal(8, \"Dark2\"))\ngrDevices::dev.off()\n```\n\n![The resulting word cloud from my course descriptions.](./img-word-cloud.png)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}